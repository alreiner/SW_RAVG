---
title: "SW RAVG nonlinear Models and Accuracies/Kappa in Kfold with RdNBR"
author: "Alicia Reiner"
date: "6/23/2021"
output: 
  html_document: 
    keep_md: yes
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Load Data and Packages

first load field data

then load PI dataset

rescale CBI data so BEINF distribution in gamlss can be used (needs 0-1)

I did not include code to calc dCC as combination of adj1.FVSVU and TreeCCloss - intend to split out PI data and run PI vs. feild data separately. 

Using RdNBR as predictor for this version

```{r Southwest RAVG}
SWRAVG_field=read.csv("RAVG_V18_field.csv",header=TRUE, sep=",")
SWRAVG_PI=read.csv("RAVG_V18_PI.csv",header=TRUE, sep=",")
SWRAVG=read.csv("RAVG_V18.csv",header=TRUE, sep=",")

CBI.B<-((1/3)*(SWRAVG_field$CBI-3)+1)
SWRAVG_field<-cbind(CBI.B,SWRAVG_field)

#categorize the CBI 0-1 data 
#SWRAVG_field$CBI.cat=cut(SWRAVG_field$CBI.B,
            #    breaks=c(-Inf,0.03333,0.41667,0.75,Inf),
             #   labels=c("0-0.1","0.1-1.25","1.25-2.25","2.25-3"))

#categorize the CBI data - use CBI on 0-3 scale for these nonlinear models 
SWRAVG_field$CBI.cat=cut(SWRAVG_field$CBI,
                breaks=c(-Inf,0.9999,1.24999,2.24999,Inf),
                labels=c("0-<0.1","0.1-<1.25","1.25-<2.25","2.25-3"))

#create a combined field and PI canopy cover which uses an average if both are present.  
SWRAVG$pdCC<-ifelse(SWRAVG$OS=="Y",((SWRAVG$adj.lim.pdFVSVU+SWRAVG$pdTreeCCloss)/2),ifelse(is.na(SWRAVG$pdTreeCCloss)=="TRUE",SWRAVG$adj.lim.pdFVSVU,SWRAVG$pdTreeCCloss))
SWRAVG_field$pdCC<-ifelse(SWRAVG_field$OS=="Y",((SWRAVG_field$adj.lim.pdFVSVU+SWRAVG_field$pdTreeCCloss)/2),ifelse(is.na(SWRAVG_field$pdTreeCCloss)=="TRUE",SWRAVG_field$adj.lim.pdFVSVU,SWRAVG_field$pdTreeCCloss))


#explore and remove outlier
plot(SWRAVG_field$W_PctGrn_Tree,SWRAVG_field$adj.lim.pdFVSVU)
SWRAVG_field<-SWRAVG_field[-which(SWRAVG_field$Plot=="BLUE11"),]
SWRAVG<-SWRAVG[-which(SWRAVG$Plot=="BLUE11"),]

summary(SWRAVG_field$adj.lim.pdFVSVU)
summary(SWRAVG_field$pdFVSVU)
summary(SWRAVG_field$pdTreeCCloss)
hist(SWRAVG_field$adj.lim.pdFVSVU)
hist(SWRAVG_field$pdFVSVU)
hist(SWRAVG_field$pdTreeCCloss)


library(ggplot2)
library("caret")
#library(dplyr)
library("plyr")
library(magrittr)
```


### Partition data for k-fold

not creating folds based on stratified random because intend to apply data to new fires

Trying to do the kfolds without the 'createFolds' function from caret because it was only assigning 12 obs to test sets.  

```{r SW RAVG create folds}

set.seed(123)

#randomly shuffle data
SWRAVG_field<-SWRAVG_field[sample(nrow(SWRAVG_field)),]

#create folds
folds_f<-cut(seq(1,nrow(SWRAVG_field)),breaks=7,labels=FALSE)

#again for PI dataset

set.seed(123)
SWRAVG_PI<-SWRAVG_PI[sample(nrow(SWRAVG_PI)),]
folds_PI<-cut(seq(1,nrow(SWRAVG_PI)),breaks=7,labels=FALSE)

#again for combined dataset

set.seed(123)
SWRAVG<-SWRAVG[sample(nrow(SWRAVG)),]
folds<-cut(seq(1,nrow(SWRAVG)),breaks=7,labels=FALSE)



```

### CBI parametric model and Confusion Matrices


```{r SW RAVG CBI}
#set up blank lists to store results
CBI.Ac<-c()
CBI.K<-c()
CBI.testMSE<-c()
CBI.AIC<-c()
lowerlimit.CBI.l<-c()
upperlimit.CBI.l<-c()


#create a blank vector to hold ifelse predictions, aka, predictions with asymptotes
CBI.pred.p.if<-999

for(i in 1:7){

#Then predict CBI on entire field/PI dataset minus reserve data.  

  #Segement your data by fold using the which() function 
    testIndexes <- which(folds_f==i,arr.ind=TRUE)
    SWRAVG_field_test <- SWRAVG_field[testIndexes, ]
    SWRAVG_field_train <- SWRAVG_field[-testIndexes, ]
  
fit.CBI<-nls(CBI~I((1/c)*log((L_EA_rdnbr_with-a)/b)), data=na.omit(SWRAVG_field_train), start=list(a=-200, b=150, c=0.4), nls.control(maxiter = 10000))

#Rather than creating predictions with standard predict statement, use if else to assign true zero and 100% change.  

#access the coefficients from the model fit above
a.fit.CBI<-summary(fit.CBI)$coefficients[1,1]
b.fit.CBI<-summary(fit.CBI)$coefficients[2,1]
c.fit.CBI<-summary(fit.CBI)$coefficients[3,1]

CBI.pred.p.if<-ifelse(SWRAVG_field_test$L_EA_rdnbr_with<(b.fit.CBI+a.fit.CBI), 0, ifelse(SWRAVG_field_test$L_EA_rdnbr_with>(a.fit.CBI+(b.fit.CBI*(exp(3*c.fit.CBI)))), 3, ((1/c.fit.CBI)*log((SWRAVG_field_test$L_EA_rdnbr_with-a.fit.CBI)/b.fit.CBI))))

print(fit.CBI)

CBI.testMSE.0<-mean((SWRAVG_field_test$CBI-CBI.pred.p.if)^2) 
CBI.testMSE<-c(CBI.testMSE,CBI.testMSE.0)
CBI.AIC.0<-AIC(fit.CBI)
CBI.AIC<-c(CBI.AIC,CBI.AIC.0)

#get regular predictions to look at things a little
CBI.pred<-predict(fit.CBI,newdata=SWRAVG_field_test)
plot(SWRAVG_field_test$L_EA_rdnbr_with,CBI.pred)

#explore limits
plot(SWRAVG_field_test$L_EA_rdnbr_with,CBI.pred.p.if)
lowerlimit.CBI=b.fit.CBI+a.fit.CBI
lowerlimit.CBI.l<-c(lowerlimit.CBI.l,lowerlimit.CBI)
upperlimit.CBI=(a.fit.CBI+(b.fit.CBI*(exp(3*c.fit.CBI))))
upperlimit.CBI.l<-c(upperlimit.CBI.l,upperlimit.CBI)

#then categorize the predicted data  
CBI.pred.p.cat=cut(CBI.pred.p.if,
                breaks=c(-Inf,0.9999,1.24999,2.24999,Inf),
                labels=c("0-<0.1","0.1-<1.25","1.25-<2.25","2.25-3"))
summary(CBI.pred.p.cat)

#Create Confusion Matrix
CBI.conf<-confusionMatrix(CBI.pred.p.cat, SWRAVG_field_test$CBI.cat,  dnn = c("Prediction", "Reference"))
print(CBI.conf)

#access accuracy and Kappa
CBI.conf.ac<-CBI.conf$overall["Accuracy"]
CBI.conf.K<-CBI.conf$overall["Kappa"]

#add accuracy and Kappa to lists
CBI.Ac<-c(CBI.Ac,CBI.conf.ac)
CBI.K<-c(CBI.K,CBI.conf.K)
}

#average the results across folds
mean(CBI.Ac)
sd(CBI.Ac)/sqrt(length(CBI.Ac))
mean(CBI.K)
sd(CBI.K)/sqrt(length(CBI.K))
mean(CBI.testMSE)
mean(CBI.AIC)
print(lowerlimit.CBI.l)
print(upperlimit.CBI.l)

#plot the model for the last of the folds just for giggles
x=seq(-200,1280,by=.1)
model.CBI<-function(x){
  x=(1/c.fit.CBI)*log((x-a.fit.CBI)/b.fit.CBI)
}
ggplot(SWRAVG_field_train,aes(x=SWRAVG_field_train$L_EA_rdnbr_with,y=SWRAVG_field_train$CBI)) + geom_point() + stat_function(fun=model.CBI)
```


### BA parametric model and Confusion Matrices

```{r SW RAVG BA}


#set up blank lists to store results
BA.Ac<-c()
BA.K<-c()
BA.testMSE<-c()
BA.AIC<-c()


#categorize the reserve pdBA data 
SWRAVG_field$pdBA.cat=cut(SWRAVG_field$pdBA,
                breaks=c(-Inf,0.0999,0.24999,0.4999,0.74999,0.8999,Inf),
                labels=c("0-<10%","10-<25%","25-<50%","50-<75%","75-<90%","90-100%"))

BA.pred.p.if<-999


for(i in 1:7){
#Segement your data by fold using the which() function 
    testIndexes <- which(folds_f==i,arr.ind=TRUE)
    SWRAVG_field_test <- SWRAVG_field[testIndexes, ]
    SWRAVG_field_train <- SWRAVG_field[-testIndexes, ]  

#parametric 
fit.BA<-nls(pdBA~I(sin((L_EA_rdnbr_with-a)/b)), data=na.omit(SWRAVG_field_train), start=list(a=120, b=700), nls.control(maxiter = 10000))

print(fit.BA)

#get regular predictions to look at things a little
BA.pred<-predict(fit.BA,newdata=SWRAVG_field_test)
plot(SWRAVG_field_test$L_EA_rdnbr_with,BA.pred)


#Rather than creating predictions with standard predict statement, use if else to assign true zero and 100% change.  

#access the coefficients from the model fit above
a.fit.BA<-summary(fit.BA)$coefficients[1,1]
b.fit.BA<-summary(fit.BA)$coefficients[2,1]


BA.pred.p.if<-ifelse(((SWRAVG_field_test$L_EA_rdnbr_with-a.fit.BA)/b.fit.BA)<=0, 0, ifelse(1.570963<((SWRAVG_field_test$L_EA_rdnbr_with-a.fit.BA)/b.fit.BA), 1, sin((SWRAVG_field_test$L_EA_rdnbr_with-a.fit.BA)/b.fit.BA)))

#explore limits
plot(SWRAVG_field_test$L_EA_rdnbr_with,BA.pred.p.if)


BA.testMSE.0<-mean((SWRAVG_field_test$pdBA-BA.pred.p.if)^2) 
BA.testMSE<-c(BA.testMSE,BA.testMSE.0)
BA.AIC.0<-AIC(fit.BA)
BA.AIC<-c(BA.AIC,BA.AIC.0)


#then categorize the predicted data  
BA.pred.p.cat=cut(BA.pred.p.if,
                breaks=c(-Inf,0.0999,0.24999,0.4999,0.74999,0.8999, Inf),
                labels=c("0-<10%","10-<25%","25-<50%","50-<75%","75-<90%","90-100%"))

#Create Confusion Matrix
BA.conf<-confusionMatrix(BA.pred.p.cat, SWRAVG_field_test$pdBA.cat,  dnn = c("Prediction", "Reference"))
print(BA.conf)

  #access accuracy and Kappa
BA.conf.ac<-BA.conf$overall["Accuracy"]
BA.conf.K<-BA.conf$overall["Kappa"]

#add accuracy and Kappa to lists
BA.Ac<-c(BA.Ac,BA.conf.ac)
BA.K<-c(BA.K,BA.conf.K)

}

#average the results across folds
mean(BA.Ac)
sd(BA.Ac)/sqrt(length(BA.Ac))
mean(BA.K)
sd(BA.K)/sqrt(length(BA.K))
mean(BA.testMSE)
mean(BA.AIC)


```


### Adjusted pdFVSVU parametric model and Confusion Matrices (with field datasets)
```{r SW RAVG FVSVU}
#set up blank lists to store results
pdFVSVU.Ac<-c()
pdFVSVU.K<-c()
pdFVSVU.testMSE<-c()
pdFVSVU.AIC<-c()



#first categorize the reserve pdFVSVU data 
SWRAVG_field$pdFVSVU.cat=cut(SWRAVG_field$pdFVSVU,
                breaks=c(-Inf,0.0999,0.24999,0.4999,0.74999,0.8999, Inf),
                labels=c("0-<10%","10-<25%","25-<50%","50-<75%","75-<90%","90-100%"))

pdFVSVU.pred.p.if<-999

for(i in 1:7){

#Segement your data by fold using the which() function 
    testIndexes <- which(folds_f==i,arr.ind=TRUE)
    SWRAVG_field_test <- SWRAVG_field[testIndexes, ]
    SWRAVG_field_train <- SWRAVG_field[-testIndexes, ]

fit.pdFVSVU.2<-nls(pdFVSVU~I(sin((L_EA_rdnbr_with-a)/b)), data=na.omit(SWRAVG_field_train), start=list(a=160, b=390), nls.control(maxiter = 10000))

print(fit.pdFVSVU.2)


#get regular predictions to look at things a little
pdFVSVU.pred<-predict(fit.pdFVSVU.2,newdata=SWRAVG_field_test)
plot(SWRAVG_field_test$L_EA_rdnbr_with,pdFVSVU.pred)


#Rather than creating predictions with standard predict statement, use if else to assign true zero and 100% change.  

#access the coefficients from the model fit above
a.fit.pdFVSVU.2<-summary(fit.pdFVSVU.2)$coefficients[1,1]
b.fit.pdFVSVU.2<-summary(fit.pdFVSVU.2)$coefficients[2,1]


#May want to replace the sattelite index variable with an anonymous function
pdFVSVU.pred.p.if<-ifelse(((SWRAVG_field_test$L_EA_rdnbr_with-a.fit.pdFVSVU.2)/b.fit.pdFVSVU.2)<=0, 0, ifelse(1.570963<((SWRAVG_field_test$L_EA_rdnbr_with-a.fit.pdFVSVU.2)/b.fit.pdFVSVU.2), 1, sin(((SWRAVG_field_test$L_EA_rdnbr_with-a.fit.pdFVSVU.2)/b.fit.pdFVSVU.2))))

pdFVSVU.testMSE.0<-mean((SWRAVG_field_test$pdFVSVU-pdFVSVU.pred.p.if)^2) 
pdFVSVU.testMSE<-c(pdFVSVU.testMSE,pdFVSVU.testMSE.0)
pdFVSVU.AIC.0<-AIC(fit.pdFVSVU.2)
pdFVSVU.AIC<-c(pdFVSVU.AIC,pdFVSVU.AIC.0)

#explore limits
plot(SWRAVG_field_test$L_EA_rdnbr_with,pdFVSVU.pred.p.if)


#then categorize the predicted data  
pdFVSVU.pred.p.cat=cut(pdFVSVU.pred.p.if,
                breaks=c(-Inf,0.0999,0.24999,0.4999,0.74999,0.8999, Inf),
                labels=c("0-<10%","10-<25%","25-<50%","50-<75%","75-<90%","90-100%"))

#Create Confusion Matrix
pdFVSVU.conf<-confusionMatrix(pdFVSVU.pred.p.cat, SWRAVG_field_test$pdFVSVU.cat,  dnn = c("Prediction", "Reference"))
print(pdFVSVU.conf)

 #access accuracy and Kappa
pdFVSVU.conf.ac<-pdFVSVU.conf$overall["Accuracy"]
pdFVSVU.conf.K<-pdFVSVU.conf$overall["Kappa"]

#add accuracy and Kappa to lists
pdFVSVU.Ac<-c(pdFVSVU.Ac,pdFVSVU.conf.ac)
pdFVSVU.K<-c(pdFVSVU.K,pdFVSVU.conf.K)

}

#average the results across folds
mean(pdFVSVU.Ac)
sd(pdFVSVU.Ac)/sqrt(length(pdFVSVU.Ac))
mean(pdFVSVU.K)
sd(pdFVSVU.K)/sqrt(length(pdFVSVU.K))
mean(pdFVSVU.testMSE)
mean(pdFVSVU.AIC)


```


### Adjusted pdFVSVU parametric model and Confusion Matrices (with field datasets)
```{r SW RAVG adjusted FVSVU}
#set up blank lists to store results
adj.lim.pdFVSVU.Ac<-c()
adj.lim.pdFVSVU.K<-c()
adj.lim.pdFVSVU.testMSE<-c()
adj.lim.pdFVSVU.AIC<-c()



#first categorize the reserve adj.lim.pdFVSVU data 
SWRAVG_field$adj.lim.pdFVSVU.cat=cut(SWRAVG_field$adj.lim.pdFVSVU,
                breaks=c(-Inf,0.0999,0.24999,0.4999,0.74999,0.8999, Inf),
                labels=c("0-<10%","10-<25%","25-<50%","50-<75%","75-<90%","90-100%"))

adj.lim.pdFVSVU.pred.p.if<-999

for(i in 1:7){

#Segement your data by fold using the which() function 
    testIndexes <- which(folds_f==i,arr.ind=TRUE)
    SWRAVG_field_test <- SWRAVG_field[testIndexes, ]
    SWRAVG_field_train <- SWRAVG_field[-testIndexes, ]

fit.adj.lim.pdFVSVU.2<-nls(adj.lim.pdFVSVU~I(sin((L_EA_rdnbr_with-a)/b)), data=na.omit(SWRAVG_field_train), start=list(a=160, b=390), nls.control(maxiter = 10000))

print(fit.adj.lim.pdFVSVU.2)


#get regular predictions to look at things a little
adj.lim.pdFVSVU.pred<-predict(fit.adj.lim.pdFVSVU.2,newdata=SWRAVG_field_test)
plot(SWRAVG_field_test$L_EA_rdnbr_with,adj.lim.pdFVSVU.pred)


#Rather than creating predictions with standard predict statement, use if else to assign true zero and 100% change.  

#access the coefficients from the model fit above
a.fit.adj.lim.pdFVSVU.2<-summary(fit.adj.lim.pdFVSVU.2)$coefficients[1,1]
b.fit.adj.lim.pdFVSVU.2<-summary(fit.adj.lim.pdFVSVU.2)$coefficients[2,1]


#May want to replace the sattelite index variable with an anonymous function
adj.lim.pdFVSVU.pred.p.if<-ifelse(((SWRAVG_field_test$L_EA_rdnbr_with-a.fit.adj.lim.pdFVSVU.2)/b.fit.adj.lim.pdFVSVU.2)<=0, 0, ifelse(1.570963<((SWRAVG_field_test$L_EA_rdnbr_with-a.fit.adj.lim.pdFVSVU.2)/b.fit.adj.lim.pdFVSVU.2), 1, sin(((SWRAVG_field_test$L_EA_rdnbr_with-a.fit.adj.lim.pdFVSVU.2)/b.fit.adj.lim.pdFVSVU.2))))

adj.lim.pdFVSVU.testMSE.0<-mean((SWRAVG_field_test$adj.lim.pdFVSVU-adj.lim.pdFVSVU.pred.p.if)^2) 
adj.lim.pdFVSVU.testMSE<-c(adj.lim.pdFVSVU.testMSE,adj.lim.pdFVSVU.testMSE.0)
adj.lim.pdFVSVU.AIC.0<-AIC(fit.adj.lim.pdFVSVU.2)
adj.lim.pdFVSVU.AIC<-c(adj.lim.pdFVSVU.AIC,adj.lim.pdFVSVU.AIC.0)

#explore limits
plot(SWRAVG_field_test$L_EA_rdnbr_with,adj.lim.pdFVSVU.pred.p.if)


#then categorize the predicted data  
adj.lim.pdFVSVU.pred.p.cat=cut(adj.lim.pdFVSVU.pred.p.if,
                breaks=c(-Inf,0.0999,0.24999,0.4999,0.74999,0.8999,Inf),
                labels=c("0-<10%","10-<25%","25-<50%","50-<75%","75-<90%","90-100%"))

#Create Confusion Matrix
adj.lim.pdFVSVU.conf<-confusionMatrix(adj.lim.pdFVSVU.pred.p.cat, SWRAVG_field_test$adj.lim.pdFVSVU.cat,  dnn = c("Prediction", "Reference"))
print(adj.lim.pdFVSVU.conf)

 #access accuracy and Kappa
adj.lim.pdFVSVU.conf.ac<-adj.lim.pdFVSVU.conf$overall["Accuracy"]
adj.lim.pdFVSVU.conf.K<-adj.lim.pdFVSVU.conf$overall["Kappa"]

#add accuracy and Kappa to lists
adj.lim.pdFVSVU.Ac<-c(adj.lim.pdFVSVU.Ac,adj.lim.pdFVSVU.conf.ac)
adj.lim.pdFVSVU.K<-c(adj.lim.pdFVSVU.K,adj.lim.pdFVSVU.conf.K)

}

#average the results across folds
mean(adj.lim.pdFVSVU.Ac)
sd(adj.lim.pdFVSVU.Ac)/sqrt(length(adj.lim.pdFVSVU.Ac))
mean(adj.lim.pdFVSVU.K)
sd(adj.lim.pdFVSVU.K)/sqrt(length(adj.lim.pdFVSVU.K))
mean(adj.lim.pdFVSVU.testMSE)
mean(adj.lim.pdFVSVU.AIC)


```




### pdTreeCCloss with kfold for PI DATASETS


```{r SW RAVG pdTreeCCloss}

#set up blank lists
pdTreeCCloss.Ac<-c()
pdTreeCCloss.K<-c()
pdTreeCCloss.testMSE<-c()
pdTreeCCloss.AIC<-c()


pdTreeCCloss.pred.p.if<-999

#first categorize the reserve data 
SWRAVG_PI$pdTreeCCloss.cat=cut(SWRAVG_PI$pdTreeCCloss,
                breaks=c(-Inf,0.0999,0.24999,0.4999,0.74999,0.8999, Inf),
                labels=c("0-<10%","10-<25%","25-<50%","50-<75%","75-<90%","90-100%"))

for(i in 1:7){
#Segement your data by fold using the which() function 
    testIndexes <- which(folds_PI==i,arr.ind=TRUE)
    SWRAVG_PI_test <- SWRAVG_PI[testIndexes, ]
    SWRAVG_PI_train <- SWRAVG_PI[-testIndexes, ]
  
#Then predict pdTreeCCloss  with entire PI dataset minus reserve data.  
fit.pdTreeCCloss.2<-nls(pdTreeCCloss~I(sin((L_EA_rdnbr_with-a)/b)), data=na.omit(SWRAVG_PI_train), start=list(a=160, b=390), nls.control(maxiter = 10000))

print(fit.pdTreeCCloss.2)

#get regular predictions to look at things a little
pdTreeCCloss.pred<-predict(fit.pdTreeCCloss.2,newdata=SWRAVG_PI_test)
plot(SWRAVG_PI_test$L_EA_rdnbr_with,pdTreeCCloss.pred)

#Rather than creating predictions with standard predict statement, use if else to assign true zero and 100% change.  

#access the coefficients from the model fit above
a.fit.pdTreeCCloss.2<-summary(fit.pdTreeCCloss.2)$coefficients[1,1]
b.fit.pdTreeCCloss.2<-summary(fit.pdTreeCCloss.2)$coefficients[2,1]


pdTreeCCloss.pred.p.if<-ifelse(((SWRAVG_PI_test$L_EA_rdnbr_with-a.fit.pdTreeCCloss.2)/b.fit.pdTreeCCloss.2)<=0, 0, ifelse(1.570963<((SWRAVG_PI_test$L_EA_rdnbr_with-a.fit.pdTreeCCloss.2)/b.fit.pdTreeCCloss.2), 1, (sin((SWRAVG_PI_test$L_EA_rdnbr_with-a.fit.pdTreeCCloss.2)/b.fit.pdTreeCCloss.2))))

#explore limits
plot(SWRAVG_PI_test$L_EA_rdnbr_with,pdTreeCCloss.pred.p.if)

pdTreeCCloss.testMSE.0<-mean((SWRAVG_PI_test$pdTreeCCloss-pdTreeCCloss.pred.p.if)^2) 
pdTreeCCloss.testMSE<-c(pdTreeCCloss.testMSE,pdTreeCCloss.testMSE.0)
pdTreeCCloss.AIC.0<-AIC(fit.pdTreeCCloss.2)
pdTreeCCloss.AIC<-c(pdTreeCCloss.AIC,pdTreeCCloss.AIC.0)

#then categorize the predicted data  
pdTreeCCloss.pred.p.cat=cut(pdTreeCCloss.pred.p.if,
                breaks=c(-Inf,0.0999,0.24999,0.4999,0.74999,0.8999, Inf),
                labels=c("0-<10%","10-<25%","25-<50%","50-<75%","75-<90%","90-100%"))

#Create Confusion Matrix
pdTreeCCloss.conf<-confusionMatrix(pdTreeCCloss.pred.p.cat, SWRAVG_PI_test$pdTreeCCloss.cat,  dnn = c("Prediction", "Reference"))
print(pdTreeCCloss.conf)

 #access accuracy and Kappa
pdTreeCCloss.conf.ac<-pdTreeCCloss.conf$overall["Accuracy"]
pdTreeCCloss.conf.K<-pdTreeCCloss.conf$overall["Kappa"]

#add accuracy and Kappa to lists
pdTreeCCloss.Ac<-c(pdTreeCCloss.Ac,pdTreeCCloss.conf.ac)
pdTreeCCloss.K<-c(pdTreeCCloss.K,pdTreeCCloss.conf.K)

}

#average the results across folds
mean(pdTreeCCloss.Ac)
sd(pdTreeCCloss.Ac)/sqrt(length(pdTreeCCloss.Ac))
mean(pdTreeCCloss.K)
sd(pdTreeCCloss.K)/sqrt(length(pdTreeCCloss.K))
mean(pdTreeCCloss.testMSE)
mean(pdTreeCCloss.AIC)


```



### pdCC with kfold for COMBINED feild and PI DATASETS


```{r SW RAVG pdCC}

#set up blank lists
pdCC.Ac<-c()
pdCC.K<-c()
pdCC.testMSE<-c()
pdCC.AIC<-c()


pdCC.pred.p.if<-999

#first categorize the reserve data 
SWRAVG$pdCC.cat=cut(SWRAVG$pdCC,
                breaks=c(-Inf,0.0999,0.24999,0.4999,0.74999,0.8999, Inf),
                labels=c("0-<10%","10-<25%","25-<50%","50-<75%","75-<90%","90-100%"))

for(i in 1:7){
#Segement your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    SWRAVG_test <- SWRAVG[testIndexes, ]
    SWRAVG_train <- SWRAVG[-testIndexes, ]
  
#Then predict pdCC  with entire PI dataset minus reserve data.  
fit.pdCC.2<-nls(pdCC~I(sin((L_EA_rdnbr_with-a)/b)), data=na.omit(SWRAVG_train), start=list(a=160, b=390), nls.control(maxiter = 10000))

print(fit.pdCC.2)

#get regular predictions to look at things a little
pdCC.pred<-predict(fit.pdCC.2,newdata=SWRAVG_test)
plot(SWRAVG_test$L_EA_rdnbr_with,pdCC.pred)

#Rather than creating predictions with standard predict statement, use if else to assign true zero and 100% change.  

#access the coefficients from the model fit above
a.fit.pdCC.2<-summary(fit.pdCC.2)$coefficients[1,1]
b.fit.pdCC.2<-summary(fit.pdCC.2)$coefficients[2,1]


pdCC.pred.p.if<-ifelse(((SWRAVG_test$L_EA_rdnbr_with-a.fit.pdCC.2)/b.fit.pdCC.2)<=0, 0, ifelse(1.570963<((SWRAVG_test$L_EA_rdnbr_with-a.fit.pdCC.2)/b.fit.pdCC.2), 1, (sin((SWRAVG_test$L_EA_rdnbr_with-a.fit.pdCC.2)/b.fit.pdCC.2))))

#explore limits
plot(SWRAVG_test$L_EA_rdnbr_with,pdCC.pred.p.if)

pdCC.testMSE.0<-mean((SWRAVG_test$pdCC-pdCC.pred.p.if)^2) 
pdCC.testMSE<-c(pdCC.testMSE,pdCC.testMSE.0)
pdCC.AIC.0<-AIC(fit.pdCC.2)
pdCC.AIC<-c(pdCC.AIC,pdCC.AIC.0)

#then categorize the predicted data  
pdCC.pred.p.cat=cut(pdCC.pred.p.if,
                breaks=c(-Inf,0.0999,0.24999,0.4999,0.74999,0.8999, Inf),
                labels=c("0-<10%","10-<25%","25-<50%","50-<75%","75-<90%","90-100%"))

#Create Confusion Matrix
pdCC.conf<-confusionMatrix(pdCC.pred.p.cat, SWRAVG_test$pdCC.cat,  dnn = c("Prediction", "Reference"))
print(pdCC.conf)

 #access accuracy and Kappa
pdCC.conf.ac<-pdCC.conf$overall["Accuracy"]
pdCC.conf.K<-pdCC.conf$overall["Kappa"]

#add accuracy and Kappa to lists
pdCC.Ac<-c(pdCC.Ac,pdCC.conf.ac)
pdCC.K<-c(pdCC.K,pdCC.conf.K)

}

#average the results across folds
mean(pdCC.Ac)
sd(pdCC.Ac)/sqrt(length(pdCC.Ac))
mean(pdCC.K)
sd(pdCC.K)/sqrt(length(pdCC.K))
mean(pdCC.testMSE)
mean(pdCC.AIC)


```
